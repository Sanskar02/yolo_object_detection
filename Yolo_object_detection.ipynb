{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "image = cv2.imread('Desktop/Udemy - Computer Vision Bootcamp with Python (OpenCV) - YOLO, SSD 2020-11/19. COURSE MATERIALS (DOWNLOADS)/1.1 computer_vision_code/car_image.jpg')\n",
    "original_width , original_height = image.shape[1], image.shape[0]\n",
    "\n",
    "image.shape\n",
    "\n",
    "# the whole model is trained in the coco dataset\n",
    "# there are 80 (90) possible output classes [classes where increase from 80 to 90]\n",
    "# 0:person - 2:car - 5:bus\n",
    "classes = ['car','person', 'bus']\n",
    "\n",
    "# darknet : Open source neural network\n",
    "neural_network = cv2.dnn.readNetFromDarknet('Downloads/yolov3.cfg','Downloads/yolov3.weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define whether we run the algorithm with CPU or with GPU\n",
    "# We are going to use CPU !!\n",
    "neural_network.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "neural_network.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly we need to change the image into BLOB (Binary large object) [0-1]\n",
    "# We represent images or data related file in blob format\n",
    "# Open cv uses bgr so we use TRUE to convert RGB in BGR  \n",
    "\n",
    "blob = cv2.dnn.blobFromImage(image, 1/255,(320,320), True)\n",
    "\n",
    "neural_network.setInput(blob)\n",
    "\n",
    "\n",
    "layer_names = neural_network.getLayerNames()\n",
    "print(layer_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yolo network has 3 output layer\n",
    "print(neural_network.getUnconnectedOutLayers()) # this will give indixes of the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_names = [layer_names[index[0]-1] for index in neural_network.getUnconnectedOutLayers()]\n",
    "print(output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(output_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = neural_network.forward(output_names)\n",
    "\n",
    "# there are 300 predicted bounding boxes - 85 is the prediction vector\n",
    "# first 5 parameters are (x,y,w,h,conf) + 80 output classes in coco dataset\n",
    "print(outputs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOLO_IMAGE_SIZE = 320\n",
    "\n",
    "def find_objects(model_outputs):\n",
    "    bounding_box_locations = []\n",
    "    class_ids = []\n",
    "    confidence_values = []\n",
    "    \n",
    "    for output in model_outputs:\n",
    "        for prediction in output:\n",
    "\n",
    "            class_probabilities = prediction[5:]\n",
    "            class_id = np.argmax(class_probabilities)\n",
    "            # confidence is the probability of detected object\n",
    "            confidence = class_probabilities[class_id]\n",
    "        \n",
    "            THRESHOLD = 0.3\n",
    "            # WE ARE NOT GOING TO BOTHER WITH OBJECTS LESS THAN 30% PROBABILITY\n",
    "        \n",
    "            if confidence > THRESHOLD:\n",
    "            \n",
    "                # x,y,w,h,conf\n",
    "                # we multiply with image size because they are in between (0-1) to get pixel value\n",
    "                w,h = int(prediction[2]*YOLO_IMAGE_SIZE), int(prediction[3]*YOLO_IMAGE_SIZE)\n",
    "                x,y = int(prediction[0]*YOLO_IMAGE_SIZE), int(prediction[1]*YOLO_IMAGE_SIZE)\n",
    "                bounding_box_locations.append([x,y,w,h])\n",
    "                class_ids.append(class_id)\n",
    "                confidence_values.append(float(confidence))\n",
    "        \n",
    "    SUPPRESSION_THRESHOLD = 0.3\n",
    "    # the lower the value: the fewer the bounding boxes will remain\n",
    "    box_indexes_to_keep = cv2.dnn.NMSBoxes(bounding_box_locations, confidence_values, THRESHOLD, SUPPRESSION_THRESHOLD)\n",
    "    \n",
    "    return box_indexes_to_keep, bounding_box_locations, class_ids, confidence_values\n",
    "\n",
    "def show_detected_images(img, bounding_box_ids, all_bounding_boxes, class_ids, confidence_values, width_ratio, height_ratio):\n",
    "    \n",
    "    for index in bounding_box_ids:\n",
    "        bounding_box = all_bounding_boxes[index[0]]\n",
    "        x,y,w,h = int(bounding_box[0]), int(bounding_box[1]), int(bounding_box[2]), int(bounding_box[3])\n",
    "        # we have to transform the location adn coordinates because the resized image\n",
    "        x = int(x*width_ratio)\n",
    "        y = int(y*height_ratio)\n",
    "        w = int(w*width_ratio)\n",
    "        h = int(h*height_ratio)\n",
    "        \n",
    "        # we are not going to detect every objects just PERSON and CAR\n",
    "        # openCV deals with BGR blue green red (255,0,0) then it is the blue color\n",
    "        if class_ids[index[0]] == 2:\n",
    "            cv2.rectangle(img, (x,y), (x+w,y+h), (255,0,0), 2)\n",
    "            class_with_confidence = 'CAR' + str(int(confidence_values[index[0]] *100)) + '%'\n",
    "            cv2.putText(img, class_with_confidence, (x, y-10), cv2.FONT_HERSHEY_COMPLEX_SMALL, 0.5, (255,0,0),1)\n",
    "        \n",
    "        if class_ids[index[0]] == 0:\n",
    "            cv2.rectangle(img, (x,y), (x+w,y+h), (255,0,0), 2)\n",
    "            class_with_confidence = 'PERSON' + str(int(confidence_values[index[0]] *100)) + '%'\n",
    "            cv2.putText(img, class_with_confidence, (x, y-10), cv2.FONT_HERSHEY_COMPLEX_SMALL, 0.5, (255,0,0),1)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_objects, bbox_locations, class_label_ids, conf_values = find_objects(outputs)\n",
    "\n",
    "show_detected_images(image, predicted_objects, bbox_locations, class_label_ids, conf_values, \n",
    "                     original_width/YOLO_IMAGE_SIZE, original_height/YOLO_IMAGE_SIZE)\n",
    "\n",
    "cv2.imshow('YOLO Algorithm', image)\n",
    "cv2.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
